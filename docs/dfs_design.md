## Network Topology
When a node first starts up, it sends a join request to the controller. If accepted, the controller will assign it a `nodeId` and send this info back as a response. Once the node has its reply, it will disconnect. From here on out the storage node will connect to the controller, send a heartbeat, wait for a response (where the controller lets it know if it's alive or dead), disconnect, sleep for a bit and then repeat.

The controller, meanwhile, listens for heartbeats on a single port, and updates the latest heartbeat times for each node. It also will periodically check through its node map to see if any nodes should be marked as dead.

Nodes are marked as dead when they have missed $3$ heartbeats. This value can be configured by changing the `MISSED` constant in _constants.go_. The `RATE` constant is how often (in seconds) a heartbeat will be sent, this is set to $5$. So if no heartbeats are received within $15$ seconds, the node is marked as dead. The `HB_CHECK` constant represents how often the controller will check on the statuses of its nodes. This ensures that the controller will be able to detect when a node process is terminated, this is set to $5$. When a node is marked as dead, the controller will remove all chunk mappings that correspond to this node.

If a node is not terminated and instead becomes slow enough that it misses $3$ heartbeats, it will be marked dead and not allowed to re-enter the system. Instead, the controller will send back a dead status response after receiving the lagged heartbeat. From here, the node will disconnect and ask to rejoin the controller where it will receive a new `nodeId`.

Each heartbeat will contain the node id, available space, number of requests successfully processed, and (if applicable) new chunk info.

The controller will also have another thread that checks the replication level of each chunk. The current replication level is $3$, meaning there should be $3$ copies of each chunk. This can be configured by adjusting the `REPLICAS` constant. This thread checks every 12 seconds, which can be configured with the `REP_CHECK` constant. Once a chunk has reached below the replication level, the controller will send out a replication order to a storage node requesting it to pass on replicas to the specified node(s).

The last constant is the `TIMEOUT` constant. The controller wants to be sure that it does not check the replication level of a chunk that is in the process of being stored (maybe the pipeline of replicas got delayed). So, we do not check the replication level until all chunks initially reach the replication level, this is where the chunk is marked as completed and can be checked on by the controller. However, in the case that the replicas actually never reach the controller during their storage, we have a timeout, which is currently set to `15` seconds. This ensures that all chunks will eventually be checked on.

## Client Requests
Cluster can handle 5 requests ~ storage, retrieval, deletion, ls, nodes
  - **Store:** when a client sends a storage storage request (map of `{chunk: size}`), the controller first checks if that file exists and then randomly selects a node for each chunk. If no space is available on any nodes for all chunks (and replicas) or the file already exists, the controller will respond with an error status. Otherwise, the controller responds with a mapping specifying the storage nodes for each chunk and their replicas. The client will then open up connections with the storage nodes specified for each chunk and ask if it can store the data. If allowed, the storage node will send an ok response back to the client and then store the chunk. Once finished, the storage node will checksum the new chunk and respond with either a SUCCESS or FAILED status. Then, the node will send the first replica to the next specified replica node, who will then also pass on the last replica to the next node. 
  - **Retrieve:** when a client sends a file retrieval request (filename), the controller checks its file map. If the file does not exist, the controller will respond with an error status. Otherwise, the controller will respond with a mapping specifying each chunk and their respective node (can be replica nodes). The client will then open up connections with the storage nodes for each chunk and ask to retrieve it. The storage node will verify the integrity of the requested chunks. If they are corrupted, the storage node will send a message to the controller letting it know that it needs a new copy. The controller will let the storage node know where it can retrieve a new copy. The storage node will then open up a connection with this storage node and request a clean copy. The storage node will then send this clean chunk to the client where the client will verify the checksums of each chunk retrieved. If valid, the client will rebuild the file.
  - **Delete:** when a client sends a deletion request, the controller will first check its file map. If the file exists, the controller will simply remove this file from the map. If this is successful, the controller will respond to the client with a SUCCESS status, if not then a FAILED status.
  - **List:** when a client sends a ls request, the controller will respond with a list of all files stored in the cluster.
  - **Nodes:** when a client sends a nodes request, the controller will respond with the remaining disk space in the cluster (GB), and a list of all active nodes and the number of requests they have handled.
